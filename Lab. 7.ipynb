{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Laboratorium 7\n",
    "\n",
    "Celem siódmego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmu głębokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm będzie testowany z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dołączenie standardowych bibliotek"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dołączenie bibliotek do obsługi sieci neuronowych"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zadanie 1 - Actor-Critic\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu należy utworzyć dwie głębokie sieci neuronowe:\n",
    "    1. *actor* - sieć, która będzie uczyła się optymalnej strategii (podobna do tej z laboratorium 6),\n",
    "    2. *critic* - sieć, która będzie uczyła się funkcji oceny stanu (podobnie jak się DQN).\n",
    "Wagi sieci *actor* aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta).\n",
    "\\end{equation*}\n",
    "Wagi sieci *critic* aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    w \\leftarrow w + \\beta \\delta_t \\nabla_w\\upsilon(s_{t + 1}, w),\n",
    "\\end{equation*}\n",
    "gdzie:\n",
    "\\begin{equation*}\n",
    "    \\delta_t \\leftarrow r_t + \\gamma \\upsilon(s_{t + 1}, w) - \\upsilon(s_t, w).\n",
    "\\end{equation*}\n",
    "</p>"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, action_size, actor, critic, policy):\n",
    "        self.gamma = 0.99\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.policy = policy\n",
    "        self.action_space = [i for i in range(action_size)]\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, basing on policy returned by the network.\n",
    "\n",
    "        Note: To pick action according to the probability generated by the network\n",
    "        \"\"\"\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get action in a given state\n",
    "        #        \n",
    "        state = state[np.newaxis, :]\n",
    "        probabilities = self.policy.predict(state)[0]\n",
    "        return np.random.choice(self.action_space, p=probabilities)\n",
    "  \n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Function learn networks using information about state, action, reward and next state. \n",
    "        First the values for state and next_state should be estimated based on output of critic network.\n",
    "        Critic network should be trained based on target value:\n",
    "        target = r + \\gamma next_state_value if not done]\n",
    "        target = r if done.\n",
    "        Actor network shpuld be trained based on delta value:\n",
    "        delta = target - state_value\n",
    "        \"\"\"\n",
    "        #\n",
    "        # INSERT CODE HERE to train network\n",
    "        #\n",
    "        \n",
    "        state = state[np.newaxis, :]\n",
    "        next_state = next_state[np.newaxis, :]\n",
    "        critic_value_for_next_state = self.critic.predict(next_state)\n",
    "        critic_value = self.critic.predict(state)\n",
    "\n",
    "        target = reward + self.gamma * critic_value_for_next_state * (1 - int(done))\n",
    "        delta = target - critic_value\n",
    "\n",
    "        actions = np.zeros([1, self.action_size])\n",
    "        actions[np.arange(1), action] = 1\n",
    "\n",
    "        self.actor.fit([state, delta], actions, verbose=0)\n",
    "\n",
    "        self.critic.fit(state, target, verbose=0)"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "alpha_learning_rate = 0.0001\n",
    "beta_learning_rate = 0.0005\n",
    "\n",
    "input = Input(shape=(state_size,))\n",
    "delta = Input(shape=[1])\n",
    "dense1 = Dense(64, activation='relu')(input)\n",
    "probs = Dense(action_size, activation='softmax')(dense1)\n",
    "values = Dense(1, activation='linear')(dense1)\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    out = K.clip(y_pred, 1e-8, 1 - 1e-8)\n",
    "    log_lik = y_true * K.log(out)\n",
    "\n",
    "    return K.sum(-log_lik * delta)\n",
    "\n",
    "\n",
    "actor_model = Model(input=[input, delta], output=[probs])\n",
    "actor_model.compile(optimizer=Adam(lr=alpha_learning_rate), loss=custom_loss)\n",
    "critic_model = Model(input=[input], output=[values])\n",
    "critic_model.compile(optimizer=Adam(lr=beta_learning_rate), loss='mean_squared_error')\n",
    "policy = Model(input=[input], output=[probs])"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/konrad/anaconda3/envs/tfgpu/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "/home/konrad/anaconda3/envs/tfgpu/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "/home/konrad/anaconda3/envs/tfgpu/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Czas nauczyć agenta gry w środowisku *CartPool*:"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "agent = ActorCriticAgent(action_size, actor_model, critic_model, policy)\n",
    "\n",
    "for i in range(100):\n",
    "    score_history = []\n",
    "\n",
    "    for i in range(100):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        for t in range(1000):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if (done):\n",
    "                break\n",
    "        score_history.append(score)\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n",
    "\n",
    "    if np.mean(score_history) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    # ~20-40 min"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "mean reward:21.420\n",
      "mean reward:20.550\n",
      "mean reward:22.290\n",
      "mean reward:29.500\n",
      "mean reward:40.530\n",
      "mean reward:54.830\n",
      "mean reward:103.020\n",
      "mean reward:158.230\n",
      "mean reward:154.460\n",
      "mean reward:227.090\n",
      "mean reward:136.600\n",
      "mean reward:101.690\n",
      "mean reward:185.940\n",
      "mean reward:176.040\n",
      "mean reward:269.780\n",
      "mean reward:976.050\n",
      "You Win!\n"
     ],
     "output_type": "stream"
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}